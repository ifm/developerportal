<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Enabling GPU usage on the VPU &mdash; O3R  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> O3R
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">O3R</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/content/installation_instructions.html">Installation instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/cli_link.html">Command Line tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/python_api.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/cpp_api/annotated.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ifm3d/doc/sphinx/index.html">Indices and tables</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">O3R</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Enabling GPU usage on the VPU</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/documentation/O3R/Docker/gpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="enabling-gpu-usage-on-the-vpu">
<h1>Enabling GPU usage on the VPU<a class="headerlink" href="#enabling-gpu-usage-on-the-vpu" title="Permalink to this headline"></a></h1>
<section id="using-the-gpu-of-the-vpu">
<h2>Using the GPU of the VPU<a class="headerlink" href="#using-the-gpu-of-the-vpu" title="Permalink to this headline"></a></h2>
<p>The VPU provides substantial GPU (Graphical Processing Unit) power to the user. The best way to experience this is using CUDA and the samples from NVIDIA. To do so, we are building a container with the sample files from NVIDIA, push it to the VPU and execute it. This, however is not enough. Docker is not using the GPU power if not specified. We need to activate this too via the right runtime.</p>
<section id="dockerfile-sample">
<h3>Dockerfile sample<a class="headerlink" href="#dockerfile-sample" title="Permalink to this headline"></a></h3>
<p>The following Dockerfile builds the container with the samples from NVIDIA (see <a class="reference external" href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery">https://github.com/NVIDIA/cuda-samples/tree/master/Samples/deviceQuery</a>).</p>
<p>Dockerfile:</p>
<div class="highlight-Docker notranslate"><div class="highlight"><pre><span></span><span class="c"># Base linux for tegra (l4t) amr64/aarch64 image</span>
<span class="k">FROM</span> <span class="s">nvcr.io/nvidia/l4t-base:r32.4.3</span> <span class="k">AS</span> <span class="s">buildstage</span>

<span class="c"># Install necessary updates + git (for cloning the nvidia samples). Tag v10.2 specifies the right commit. VPU runs CUDA 10.2</span>
<span class="k">RUN</span> apt-get update <span class="o">&amp;&amp;</span> apt-get install -y --no-install-recommends make g++ git <span class="o">&amp;&amp;</span> apt-get install ca-certificates -y
<span class="k">RUN</span> git clone --depth <span class="m">1</span> --branch v10.2 https://github.com/NVIDIA/cuda-samples.git /tmp/

<span class="c"># Change into the right directory and install/make the samples</span>
<span class="k">WORKDIR</span><span class="s"> /tmp/Samples/deviceQuery</span>
<span class="k">RUN</span> make clean <span class="o">&amp;&amp;</span> make

<span class="c"># Multistage build to reduce the image size on the platform</span>
<span class="k">FROM</span> <span class="s">nvcr.io/nvidia/l4t-base:r32.4.3</span>

<span class="c"># Copy the samples from the buildstage into the final image</span>
<span class="k">RUN</span> mkdir -p /usr/local/bin
<span class="k">COPY</span> --from<span class="o">=</span>buildstage /tmp/Samples/deviceQuery/deviceQuery /usr/local/bin

<span class="c"># Execute the deviceQuery and check for CUDA support. Don&#39;t forget the runtime with the docker run command</span>
<span class="k">CMD</span> <span class="p">[</span><span class="s2">&quot;/usr/local/bin/deviceQuery&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Building the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker image build . -t cuda-samples
<span class="go">Sending build context to Docker daemon  875.5MB</span>
<span class="go">Step 1/9 : FROM nvcr.io/nvidia/l4t-base:r32.4.3 AS buildstage</span>
<span class="go"> ---&gt; c93fc89026d9</span>
<span class="go">...</span>
<span class="go">Successfully tagged cuda-samples:latest</span>
</pre></div>
</div>
<p>After building the container, you can follow the steps from the documentation to test the container on the VPU:</p>
<ul class="simple">
<li><p><a class="reference external" href="../../GeneralDoc/docker.md#saving-a-container">Save the container</a>: <code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">docker</span> <span class="pre">save</span> <span class="pre">cuda-samples</span> <span class="pre">&gt;</span> <span class="pre">cuda-samples.tar</span></code></p></li>
<li><p><a class="reference external" href="../../GeneralDoc/docker.md#scp">Transfer the container</a>: <code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">scp</span> <span class="pre">cuda-samples.tar</span> <span class="pre">oem&#64;192.168.0.69:/home/oem</span></code></p></li>
<li><p><a class="reference external" href="../../GeneralDoc/docker.md#load-and-start-container">Load the container</a>: <code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">docker</span> <span class="pre">load</span> <span class="pre">&lt;</span> <span class="pre">cuda-samples.tar</span></code></p></li>
</ul>
</section>
<section id="start-the-container-with-the-nvidia-runtime">
<h3>Start the container with the NVIDIA runtime<a class="headerlink" href="#start-the-container-with-the-nvidia-runtime" title="Permalink to this headline"></a></h3>
<p>To use CUDA and the GPU, you have to specify the NVIDIA runtime, either with the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command, or within the <code class="docutils literal notranslate"><span class="pre">docker-compose.yml</span></code> (see <span class="xref myst">autostart</span>).</p>
<section id="using-docker-run">
<h4>Using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code><a class="headerlink" href="#using-docker-run" title="Permalink to this headline"></a></h4>
<p>Use the <code class="docutils literal notranslate"><span class="pre">--runtime</span> <span class="pre">nvidia</span></code> argument when running your container. The output of the running container should look similar to this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">o3r-vpu-c0:~$ </span>docker run -it --runtime nvidia cuda-samples
<span class="go">/usr/local/bin/deviceQuery Starting...</span>

<span class="go"> CUDA Device Query (Runtime API) version (CUDART static linking)</span>

<span class="go">Detected 1 CUDA Capable device(s)</span>

<span class="go">Device 0: &quot;NVIDIA Tegra X2&quot;</span>
<span class="go">  CUDA Driver Version / Runtime Version          10.2 / 10.2</span>
<span class="go">  CUDA Capability Major/Minor version number:    6.2</span>
<span class="go">  Total amount of global memory:                 3829 MBytes (4014751744 bytes)</span>
<span class="go">  ( 2) Multiprocessors, (128) CUDA Cores/MP:     256 CUDA Cores</span>
<span class="go">  GPU Max Clock rate:                            1300 MHz (1.30 GHz)</span>
<span class="go">  Memory Clock rate:                             1300 Mhz</span>
<span class="go">  Memory Bus Width:                              128-bit</span>
<span class="go">  L2 Cache Size:                                 524288 bytes</span>
<span class="go">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span>
<span class="go">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span>
<span class="go">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span>
<span class="go">  Total amount of constant memory:               65536 bytes</span>
<span class="go">  Total amount of shared memory per block:       49152 bytes</span>
<span class="go">  Total number of registers available per block: 32768</span>
<span class="go">  Warp size:                                     32</span>
<span class="go">  Maximum number of threads per multiprocessor:  2048</span>
<span class="go">  Maximum number of threads per block:           1024</span>
<span class="go">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span>
<span class="go">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span>
<span class="go">  Maximum memory pitch:                          2147483647 bytes</span>
<span class="go">  Texture alignment:                             512 bytes</span>
<span class="go">  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)</span>
<span class="go">  Run time limit on kernels:                     No</span>
<span class="go">  Integrated GPU sharing Host Memory:            Yes</span>
<span class="go">  Support host page-locked memory mapping:       Yes</span>
<span class="go">  Alignment requirement for Surfaces:            Yes</span>
<span class="go">  Device has ECC support:                        Disabled</span>
<span class="go">  Device supports Unified Addressing (UVA):      Yes</span>
<span class="go">  Device supports Compute Preemption:            Yes</span>
<span class="go">  Supports Cooperative Kernel Launch:            Yes</span>
<span class="go">  Supports MultiDevice Co-op Kernel Launch:      Yes</span>
<span class="go">  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 0</span>
<span class="go">  Compute Mode:</span>
<span class="go">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span>

<span class="go">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1</span>
<span class="go">Result = PASS</span>
</pre></div>
</div>
<p>Note that starting the container without the runtime leads to a FAIL:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">o3r-vpu-c0:~$ </span>docker run -it cuda
<span class="go">/usr/local/bin/deviceQuery Starting...</span>

<span class="go"> CUDA Device Query (Runtime API) version (CUDART static linking)</span>

<span class="go">cudaGetDeviceCount returned 35</span>
<span class="go">-&gt; CUDA driver version is insufficient for CUDA runtime version</span>
<span class="go">Result = FAIL</span>
</pre></div>
</div>
</section>
<section id="use-docker-compose-to-specify-the-runtime">
<h4>Use <code class="docutils literal notranslate"><span class="pre">docker-compose</span></code> to specify the runtime<a class="headerlink" href="#use-docker-compose-to-specify-the-runtime" title="Permalink to this headline"></a></h4>
<p>Specifying the runtime in <code class="docutils literal notranslate"><span class="pre">docker-compose.yml</span></code> is possible for versions above <code class="docutils literal notranslate"><span class="pre">version:</span> <span class="pre">&quot;2.3&quot;</span></code> to get the runtime argument.</p>
<div class="highlight-yml notranslate"><div class="highlight"><pre><span></span>version: &quot;2.3&quot;
services:
    cuda:
        image: cuda
        runtime: nvidia
</pre></div>
</div>
<p>Start the container using <code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">o3r-vpu-c0:~$ </span>docker-compose up
<span class="go">Creating network &quot;oem_default&quot; with the default driver</span>
<span class="go">Creating oem_cuda_1 ... done</span>
<span class="go">Attaching to oem_cuda_1</span>
<span class="go">cuda_1  | /usr/local/bin/deviceQuery Starting...</span>
<span class="go">cuda_1  |</span>
<span class="go">cuda_1  |  CUDA Device Query (Runtime API) version (CUDART static linking)</span>
<span class="go">cuda_1  |</span>
<span class="go">cuda_1  | Detected 1 CUDA Capable device(s)</span>
<span class="go">cuda_1  |</span>
<span class="go">cuda_1  | Device 0: &quot;NVIDIA Tegra X2&quot;</span>
<span class="go">cuda_1  |   CUDA Driver Version / Runtime Version          10.2 / 10.2</span>
<span class="go">cuda_1  |   CUDA Capability Major/Minor version number:    6.2</span>
<span class="go">cuda_1  |   Total amount of global memory:                 3829 MBytes (4014751744 bytes)</span>
<span class="go">cuda_1  |   ( 2) Multiprocessors, (128) CUDA Cores/MP:     256 CUDA Cores</span>
<span class="go">cuda_1  |   GPU Max Clock rate:                            1300 MHz (1.30 GHz)</span>
<span class="go">cuda_1  |   Memory Clock rate:                             1300 Mhz</span>
<span class="go">cuda_1  |   Memory Bus Width:                              128-bit</span>
<span class="go">cuda_1  |   L2 Cache Size:                                 524288 bytes</span>
<span class="go">cuda_1  |   Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span>
<span class="go">cuda_1  |   Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span>
<span class="go">cuda_1  |   Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span>
<span class="go">cuda_1  |   Total amount of constant memory:               65536 bytes</span>
<span class="go">cuda_1  |   Total amount of shared memory per block:       49152 bytes</span>
<span class="go">cuda_1  |   Total number of registers available per block: 32768</span>
<span class="go">cuda_1  |   Warp size:                                     32</span>
<span class="go">cuda_1  |   Maximum number of threads per multiprocessor:  2048</span>
<span class="go">cuda_1  |   Maximum number of threads per block:           1024</span>
<span class="go">cuda_1  |   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span>
<span class="go">cuda_1  |   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span>
<span class="go">cuda_1  |   Maximum memory pitch:                          2147483647 bytes</span>
<span class="go">cuda_1  |   Texture alignment:                             512 bytes</span>
<span class="go">cuda_1  |   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)</span>
<span class="go">cuda_1  |   Run time limit on kernels:                     No</span>
<span class="go">cuda_1  |   Integrated GPU sharing Host Memory:            Yes</span>
<span class="go">cuda_1  |   Support host page-locked memory mapping:       Yes</span>
<span class="go">cuda_1  |   Alignment requirement for Surfaces:            Yes</span>
<span class="go">cuda_1  |   Device has ECC support:                        Disabled</span>
<span class="go">cuda_1  |   Device supports Unified Addressing (UVA):      Yes</span>
<span class="go">cuda_1  |   Device supports Compute Preemption:            Yes</span>
<span class="go">cuda_1  |   Supports Cooperative Kernel Launch:            Yes</span>
<span class="go">cuda_1  |   Supports MultiDevice Co-op Kernel Launch:      Yes</span>
<span class="go">cuda_1  |   Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 0</span>
<span class="go">cuda_1  |   Compute Mode:</span>
<span class="go">cuda_1  |      &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span>
<span class="go">cuda_1  |</span>
<span class="go">cuda_1  | deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.2, NumDevs = 1</span>
<span class="go">cuda_1  | Result = PASS</span>
<span class="go">oem_cuda_1 exited with code 0</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, ifm CSR.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>