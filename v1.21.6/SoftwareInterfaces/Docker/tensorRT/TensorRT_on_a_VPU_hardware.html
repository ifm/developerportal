<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using TensorRT &mdash; O3R  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=a5c4661c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=5800d392" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../_static/copybutton.js?v=d9f55abb"></script>
        <script src="../../../_static/js/versionwarning.js?v=d4224a34"></script>
        <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Diagnostic" href="../../ifmDiagnostic/index_diagnostic.html" />
    <link rel="prev" title="Enabling GPU usage on the VPU" href="../gpu.html" />
    <!-- Google tag (gtag.js) -->
    <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZZ08KXKV1G"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-ZZ08KXKV1G');
    </script> -->
     

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            O3R
          </a>
              <div class="version">
                v1.21.6
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../GettingStarted/index_getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Firmware/index.html">Firmware</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../CompatibilityMatrix/compatibility_matrix.html">Compatibility Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Technology/index_technology.html">Technology</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index_software_interfaces.html">Software Interfaces</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../iVA/index_iVA.html">ifm Vision Assistant</a></li>
<li class="toctree-l2"><a class="reference external" href="https://api.ifm3d.com/stable/">ifm3d/ifm3dpy API</a></li>
<li class="toctree-l2"><a class="reference external" href="https://ros.ifm3d.com/latest/">ROS: ifm3d-ros</a></li>
<li class="toctree-l2"><a class="reference external" href="https://ros2.ifm3d.com/latest/">ROS2: ifm3d-ros2</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index_docker.html">Docker</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../docker.html">Getting started with Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="../deployVPU.html">Deployment on board</a></li>
<li class="toctree-l3"><a class="reference internal" href="../logging.html">Docker logging configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../autostart.html">Autostarting the container</a></li>
<li class="toctree-l3"><a class="reference internal" href="../resource_management.html">Resource Management on the VPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu.html">Enabling GPU usage</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">TensortRT: DL / ML model deployment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#building-a-tensorrt-container">Building a TensorRT container</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#nvidia-base-containers">NVIDIA base containers</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#compatibility-matrix">Compatibility Matrix</a></li>
<li class="toctree-l6"><a class="reference internal" href="#verify-the-functionality">Verify the functionality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#using-tensorrt-in-a-container-on-the-vpu">Using TensorRT in a container on the VPU</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#adaptations-for-the-ovp8xx-architecture">Adaptations for the OVP8xx architecture</a></li>
<li class="toctree-l5"><a class="reference internal" href="#runtime-inference-cycle-times">Runtime inference cycle times</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#calculating-the-inference-on-ovp81x-vpu-using-yolov11-onnx-model-file">Calculating the inference on OVP81x VPU using YOLOv11 ONNX Model file</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#example-runs">Example runs</a></li>
<li class="toctree-l5"><a class="reference internal" href="#inference-timings">Inference timings</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#deepstream-l4t">Deepstream-l4t</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../ifmDiagnostic/index_diagnostic.html">Diagnostic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Toolbox/index_toolbox.html">Toolbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Examples/examples.html">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../CalibrationRoutines/index_calibrations.html">Calibration Routines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ODS/index_ods.html">ODS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../PDS/index_pds.html">PDS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../PLC/index_plc.html">PLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../FAQ/FAQ.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../downloadable/index.html">History</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">O3R</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index_software_interfaces.html">Software Interfaces</a></li>
          <li class="breadcrumb-item"><a href="../index_docker.html">Docker on O3R</a></li>
      <li class="breadcrumb-item active">Using TensorRT</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/SoftwareInterfaces/Docker/tensorRT/TensorRT_on_a_VPU_hardware.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-tensorrt">
<h1>Using TensorRT<a class="headerlink" href="#using-tensorrt" title="Link to this heading"></a></h1>
<p>This document outlines the general process of AI inference acceleration with TensorRT on an OVP8xx device.</p>
<section id="building-a-tensorrt-container">
<h2>Building a TensorRT container<a class="headerlink" href="#building-a-tensorrt-container" title="Link to this heading"></a></h2>
<p>There are two options:</p>
<ul class="simple">
<li><p>Use a base NVIDIA container and import the runtime libraries directly from the firmware. This is the preferred method that we will describe below.</p></li>
<li><p>Use a complete NVIDIA container that includes the TensorRT libraries directly. This is not recommended since containers sizes will increase dramatically.</p></li>
</ul>
<section id="nvidia-base-containers">
<h3>NVIDIA base containers<a class="headerlink" href="#nvidia-base-containers" title="Link to this heading"></a></h3>
<p>NVIDIA provides L4T-based containers with TensorFlow that can be downloaded directly from <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:l4t-tensorflow">their containers catalog</a>.
TensorFlow should be used with the corresponding recommended version of JetPack.
The recommendations can be found on the <a class="reference external" href="https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform-release-notes/tf-jetson-rel.html">TensorFlow for Jetson website</a>.</p>
<section id="compatibility-matrix">
<h4>Compatibility Matrix<a class="headerlink" href="#compatibility-matrix" title="Link to this heading"></a></h4>
<table>
  <thead>
    <tr>
      <th>VPU Hardware</th>
      <th>VPU Firmware</th>
      <th>L4T Version</th>
      <th>Jetpack Version</th>
      <th>Tensorflow</th>
      <th>Pytorch</th>
      <th>Machine learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OVP81x</td>
      <td>1.20.29</td>
      <td><a href="https://developer.nvidia.com/embedded/linux-tegra-r32.7.6">R32.7.6</a></td>
      <td>4.6.5</td>
      <td>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf2.7-py3 </br>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf1.15-py3 </br>
      </td>
      <td>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3 </br>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.9-py3 </br>
      </td>
      <td>nvcr.io/nvidia/l4t-ml:r32.7.1-py3</td>
    </tr>
    <tr>
      <td>OVP81x</td>
      <td>1.10.13</td>
      <td><a href="https://developer.nvidia.com/embedded/linux-tegra-r32.7.5">R32.7.5</a></td>
      <td>4.6.5</td>
      <td>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf2.7-py3 </br>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf1.15-py3 </br>
      </td>
      <td>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3 </br>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.9-py3 </br>
      </td>
      <td>nvcr.io/nvidia/l4t-ml:r32.7.1-py3</td>
    </tr>
    <tr>
      <td>OVP81x</td>
      <td>1.4.30</td>
      <td><a href="https://developer.nvidia.com/embedded/linux-tegra-r32.7.3">R32.7.3</a></td>
      <td>4.6.3</td>
      <td>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf2.7-py3 </br>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.7.1-tf1.15-py3 </br>
      </td>
      <td>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.10-py3 </br>
        <br> nvcr.io/nvidia/l4t-pytorch:r32.7.1-pth1.9-py3 </br>
      </td>
      <td>nvcr.io/nvidia/l4t-ml:r32.7.1-py3</td>
    </tr>
    <tr>
      <td>OVP80x</td>
      <td>1.4.32</td>
      <td><a href="https://developer.nvidia.com/embedded/linux-tegra-r32.4.3">R32.4.3</a></td>
      <td>4.4.0</td>
      <td>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf2.2-py3 </br>
        <br> nvcr.io/nvidia/l4t-tensorflow:r32.4.3-tf1.15-py3 </br>
      </td>
      <td>nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3</td>
      <td>nvcr.io/nvidia/l4t-ml:r32.4.3-py3</td>
    </tr>
  </tbody>
</table>
<p>The underlying structure of the container loads the TensorRT libraries and is handled by NVIDIA and Docker - as long as the versions of the container and JetPack closely match.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To access or modify the Dockerfiles and scripts used to build the NVIDIA containers, see this <a class="reference external" href="https://github.com/dusty-nv/jetson-containers">GitHub repository</a></p>
</div>
</section>
<section id="verify-the-functionality">
<h4>Verify the functionality<a class="headerlink" href="#verify-the-functionality" title="Link to this heading"></a></h4>
<p>Start an interactive session in the container and try to import torch in interactive python as shown below.
This assumes that the containers has previously been deployed on to the VPU.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ovp81x-fc-6c-6d:~$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-ti<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>nvcr.io/nvidia/l4t-ml:r32.7.1-py3
allow<span class="w"> </span><span class="m">10</span><span class="w"> </span>sec<span class="w"> </span><span class="k">for</span><span class="w"> </span>JupyterLab<span class="w"> </span>to<span class="w"> </span>start<span class="w"> </span>@<span class="w"> </span>http://172.17.0.2:8888<span class="w"> </span><span class="o">(</span>password<span class="w"> </span>nvidia<span class="o">)</span>
JupterLab<span class="w"> </span>logging<span class="w"> </span>location:<span class="w">  </span>/var/log/jupyter.log<span class="w">  </span><span class="o">(</span>inside<span class="w"> </span>the<span class="w"> </span>container<span class="o">)</span>
root@89e52a1dfd4c:/#<span class="w"> </span>python3
Python<span class="w"> </span><span class="m">3</span>.6.9<span class="w"> </span><span class="o">(</span>default,<span class="w"> </span>Dec<span class="w">  </span><span class="m">8</span><span class="w"> </span><span class="m">2021</span>,<span class="w"> </span><span class="m">21</span>:08:43<span class="o">)</span><span class="w"> </span>
<span class="o">[</span>GCC<span class="w"> </span><span class="m">8</span>.4.0<span class="o">]</span><span class="w"> </span>on<span class="w"> </span>linux
Type<span class="w"> </span><span class="s2">&quot;help&quot;</span>,<span class="w"> </span><span class="s2">&quot;copyright&quot;</span>,<span class="w"> </span><span class="s2">&quot;credits&quot;</span><span class="w"> </span>or<span class="w"> </span><span class="s2">&quot;license&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>more<span class="w"> </span>information.
&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>torch
&gt;&gt;&gt;<span class="w"> </span>torch.cuda.is_available<span class="o">()</span>
True
&gt;&gt;&gt;<span class="w"> </span>torch.cuda.device_count<span class="o">()</span>
<span class="m">1</span>
&gt;&gt;&gt;<span class="w"> </span>torch.cuda.current_device<span class="o">()</span>
<span class="m">0</span>
&gt;&gt;&gt;<span class="w"> </span>torch.cuda.get_device_name<span class="o">(</span><span class="m">0</span><span class="o">)</span>
<span class="s1">&#39;NVIDIA Tegra X2&#39;</span>
&gt;&gt;&gt;<span class="w"> </span>
</pre></div>
</div>
<p>To mount scripts, data, etc. from your Jetson’s filesystem to run inside the container, use Docker’s <code class="docutils literal notranslate"><span class="pre">-v</span></code> flag when starting your Docker instance:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>--network<span class="w"> </span>host<span class="w"> </span>-v<span class="w"> </span>/home/user/project:/location/in/container<span class="w"> </span>nvidia<span class="w"> </span>nvcr.io/nvidia/l4t-ml:r32.7.1-py3
</pre></div>
</div>
</section>
</section>
</section>
<section id="using-tensorrt-in-a-container-on-the-vpu">
<h2>Using TensorRT in a container on the VPU<a class="headerlink" href="#using-tensorrt-in-a-container-on-the-vpu" title="Link to this heading"></a></h2>
<p>TensorRT applications can be memory-intensive. Here’s how you can manage memory effectively:</p>
<ol class="arabic simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">l4t-cuda-base</span></code> image and build TensorRT inside the container using Dockerfile. We recommend using Docker’s multistage build feature to reduce the size in the final container.</p></li>
<li><p>Reduce the container mounting size by using <a class="reference external" href="https://docs.docker.com/engine/reference/builder/#dockerignore-file">.dockerignore file</a>.
Follow the <a class="reference external" href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">Dockerfile best practices</a> to minimize the number of layers and overall size.</p></li>
</ol>
<p>Once the docker container is deployed on the VPU, you can proceed as follows:</p>
<ol class="arabic simple">
<li><p>Run TensorRT models using <code class="docutils literal notranslate"><span class="pre">trtexec</span></code> inside the <code class="docutils literal notranslate"><span class="pre">l4t-base</span></code> container. This container will copy TensorRT from the host.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trtexec</span></code> runs the model, filing it with random data for testing purposes. This is a first indication whether the adapted model can be run on the final architecture.</p></li>
<li><p>Adapt the model for the final deployment architecture. This may involve updating the model based on its structure and the operators and layers used. Not all operators and model adaptations may be available in the OVP8xx JetPack version. You may need to update your model on your development machine, export a new ONNX model with opset 11 operators, and adapt it again. This could be an iterative process.</p></li>
</ol>
<section id="adaptations-for-the-ovp8xx-architecture">
<h3>Adaptations for the OVP8xx architecture<a class="headerlink" href="#adaptations-for-the-ovp8xx-architecture" title="Link to this heading"></a></h3>
<p>The model has to be exported and adapted to the final deployment architecture.
Refer to the <a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#basic-workflow">NVIDIA documentation for this process</a>. This adaptation must be done on the final deployment architecture. Compiling on similar architectures, like Jetson evaluation boards, will result in an incompatible instruction set for the OVP8xx architecture.</p>
<p>We recommend exporting the neural network model to an ONNX model. Adapting the model for the deployment architecture may require updates. This could be an iterative process to get the model running on the final architecture. Update your model on your development machine, export a new ONNX model with opset 11 operators, and test this update in Docker.</p>
<p>For ONNX exports with opset 11 settings and further ONNX operator support, refer to the <a class="reference external" href="https://github.com/onnx/onnx-tensorrt/blob/release/7.1/operators.md">official <code class="docutils literal notranslate"><span class="pre">onnx-tensorrt</span></code> documentation</a>.</p>
</section>
<section id="runtime-inference-cycle-times">
<h3>Runtime inference cycle times<a class="headerlink" href="#runtime-inference-cycle-times" title="Link to this heading"></a></h3>
<p>Adapting the model as described will result in a model with a specific runtime on the VPU. You may need to adjust for different model sizes and operations. Remember, the typical cycle time on a development machine may not accurately reflect the expected cycle times on OVP8xx (TX2/TX2-NX) hardware.</p>
</section>
</section>
<section id="calculating-the-inference-on-ovp81x-vpu-using-yolov11-onnx-model-file">
<h2>Calculating the inference on OVP81x VPU using YOLOv11 ONNX Model file<a class="headerlink" href="#calculating-the-inference-on-ovp81x-vpu-using-yolov11-onnx-model-file" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Pull the machine learning base image provided by NVIDIA</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w">  </span>$<span class="w"> </span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/l4t-ml:r32.7.1-py3
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Create a YOLOv11 ONNX model file using python script</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>
  
  <span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">&quot;yolo11n.pt&quot;</span><span class="p">)</span> <span class="c1"># Load a pretrained YOLO model (recommended for training)</span>
  
  <span class="n">model</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">&quot;onnx&quot;</span><span class="p">,</span> <span class="n">imgsz</span><span class="o">=</span><span class="p">[</span><span class="mi">480</span><span class="p">,</span><span class="mi">640</span><span class="p">])</span> <span class="c1"># Export the model to ONNX format</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Copy the Docker container and ONNX model file to VPU</p></li>
<li><p>Run the docker image in interactive mode</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>-it<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>--gpus<span class="w"> </span>all<span class="w"> </span>-v<span class="w"> </span>/path/to/your/model:/workspace/model<span class="w"> </span>nvcr.io/nvidia/l4t-ml:r32.7.1-py3
</pre></div>
</div>
<section id="example-runs">
<h3>Example runs<a class="headerlink" href="#example-runs" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Run the command inside a docker container to measure inference timings</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>usr/src/tensorrt/bin/trtexec<span class="w"> </span>--onnx<span class="o">=</span>yolov11/yolov11n.onnx<span class="w"> </span>--verbose<span class="w"> </span>--fp16
</pre></div>
</div>
</section>
<section id="inference-timings">
<h3>Inference timings<a class="headerlink" href="#inference-timings" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Batch Size</p></th>
<th class="head"><p>Precision</p></th>
<th class="head"><p>Inference time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>YOLOv11-N</p></td>
<td><p>1</p></td>
<td><p>FP16</p></td>
<td><p>20.62 ms</p></td>
</tr>
<tr class="row-odd"><td><p>YOLOv11-M</p></td>
<td><p>1</p></td>
<td><p>FP16</p></td>
<td><p>93.24 ms</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="deepstream-l4t">
<h2>Deepstream-l4t<a class="headerlink" href="#deepstream-l4t" title="Link to this heading"></a></h2>
<p>The Deepstream-l4t NGC container is used in this example.</p>
<ol class="arabic simple">
<li><p>Pull the Deepstream-l4t NGC container.</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/deepstream-l4t:5.1-21.02-samples
</pre></div>
</div>
<ol class="arabic" start="2">
<li><p>Verify the successful pull by listing the Docker images.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>image<span class="w"> </span>ls
REPOSITORY<span class="w">                          </span>TAG<span class="w">                         </span>IMAGE<span class="w"> </span>ID<span class="w">             </span>CREATED<span class="w">             </span>SIZE
nvcr.io/nvidia/deepstream-l4t<span class="w">       </span><span class="m">5</span>.1-21.02-samples<span class="w">           </span>0ff77669c10<span class="w">          </span><span class="m">6</span><span class="w"> </span>months<span class="w"> </span>ago<span class="w">        </span><span class="m">2</span>.72GB
</pre></div>
</div>
</li>
<li><p>Start the container on the VPU: please replace the mounted volume directory with your directory of choice containing the ONNX model</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>docker<span class="w"> </span>container<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>--net<span class="o">=</span>host<span class="w"> </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span>-v<span class="w"> </span>/home/jetsontx2/for_container/:/home/dl_models<span class="w"> </span>nvcr.io/nvidia/deepstream-l4t:5.1-21.02-samples<span class="w"> </span>bash
</pre></div>
</div>
</li>
<li><p>In the container, navigate to <code class="docutils literal notranslate"><span class="pre">/home/dl_models</span> <span class="pre">directory</span></code> and run <code class="docutils literal notranslate"><span class="pre">trtexec</span></code> with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>/usr/src/tensorrt/bin/trtexec<span class="w"> </span>--onnx<span class="o">=</span>/home/dl_models/yolov4tiny_relu_best_ops12_fp32.onnx<span class="w"> </span>--fp16<span class="w"> </span>--explicitBatch<span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</li>
<li><p>Optimal performance is achieved by using fp16 (floating point 16) precision.
For TX2 board, the compute capability is 6.2 (that is SM62 architecture), which does not have INT8 feature.
The output of trtexec for Yolov4 Tiny network and fp16 precision is as below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>root@jetsontx2-desktop:/home/dl_models#<span class="w"> </span>/usr/src/tensorrt/bin/trtexec<span class="w"> </span>--onnx<span class="o">=</span>/home/dl_models/yolov4tiny_relu_best_ops12_fp32.onnx<span class="w"> </span>--fp16<span class="w"> </span>--explicitBatch<span class="o">=</span><span class="m">1</span>
<span class="o">&amp;&amp;&amp;&amp;</span><span class="w"> </span>RUNNING<span class="w"> </span>TensorRT.trtexec<span class="w"> </span><span class="c1"># /usr/src/tensorrt/bin/trtexec --onnx=/home/dl_models/yolov4tiny_relu_best_ops12_fp32.onnx --fp16 --explicitBatch=1</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">===</span><span class="w"> </span>Model<span class="w"> </span><span class="nv">Options</span><span class="w"> </span><span class="o">===</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Format:<span class="w"> </span>ONNX
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Model:<span class="w"> </span>/home/dl_models/yolov4tiny_relu_best_ops12_fp32.onnx
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Output:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">===</span><span class="w"> </span>Build<span class="w"> </span><span class="nv">Options</span><span class="w"> </span><span class="o">===</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Max<span class="w"> </span>batch:<span class="w"> </span>explicit
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Workspace:<span class="w"> </span><span class="m">16</span><span class="w"> </span>MB
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>minTiming:<span class="w"> </span><span class="m">1</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>avgTiming:<span class="w"> </span><span class="m">8</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Precision:<span class="w"> </span>FP32+FP16
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Calibration:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Safe<span class="w"> </span>mode:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Save<span class="w"> </span>engine:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Load<span class="w"> </span>engine:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Builder<span class="w"> </span>Cache:<span class="w"> </span>Enabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>NVTX<span class="w"> </span>verbosity:<span class="w"> </span><span class="m">0</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Inputs<span class="w"> </span>format:<span class="w"> </span>fp32:CHW
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Outputs<span class="w"> </span>format:<span class="w"> </span>fp32:CHW
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Input<span class="w"> </span>build<span class="w"> </span>shapes:<span class="w"> </span>model
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Input<span class="w"> </span>calibration<span class="w"> </span>shapes:<span class="w"> </span>model
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">===</span><span class="w"> </span>System<span class="w"> </span><span class="nv">Options</span><span class="w"> </span><span class="o">===</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Device:<span class="w"> </span><span class="m">0</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>DLACore:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Plugins:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">===</span><span class="w"> </span>Inference<span class="w"> </span><span class="nv">Options</span><span class="w"> </span><span class="o">===</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Batch:<span class="w"> </span>Explicit
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Input<span class="w"> </span>inference<span class="w"> </span>shapes:<span class="w"> </span>model
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Iterations:<span class="w"> </span><span class="m">10</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Duration:<span class="w"> </span>3s<span class="w"> </span><span class="o">(</span>+<span class="w"> </span>200ms<span class="w"> </span>warm<span class="w"> </span>up<span class="o">)</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Sleep<span class="w"> </span>time:<span class="w"> </span>0ms
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Streams:<span class="w"> </span><span class="m">1</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>ExposeDMA:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Spin-wait:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Multithreading:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>CUDA<span class="w"> </span>Graph:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Skip<span class="w"> </span>inference:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Inputs:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">===</span><span class="w"> </span>Reporting<span class="w"> </span><span class="nv">Options</span><span class="w"> </span><span class="o">===</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Verbose:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Averages:<span class="w"> </span><span class="m">10</span><span class="w"> </span>inferences
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Percentile:<span class="w"> </span><span class="m">99</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Dump<span class="w"> </span>output:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Profile:<span class="w"> </span>Disabled
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Export<span class="w"> </span>timing<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>file:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Export<span class="w"> </span>output<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>file:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Export<span class="w"> </span>profile<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>file:
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:45<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span>
----------------------------------------------------------------
Input<span class="w"> </span>filename:<span class="w">   </span>/home/dl_models/yolov4tiny_relu_best_ops12_fp32.onnx
ONNX<span class="w"> </span>IR<span class="w"> </span>version:<span class="w">  </span><span class="m">0</span>.0.6
Opset<span class="w"> </span>version:<span class="w">    </span><span class="m">12</span>
Producer<span class="w"> </span>name:<span class="w">    </span>pytorch
Producer<span class="w"> </span>version:<span class="w"> </span><span class="m">1</span>.8
Domain:
Model<span class="w"> </span>version:<span class="w">    </span><span class="m">0</span>
Doc<span class="w"> </span>string:
----------------------------------------------------------------
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>onnx2trt_utils.cpp:220:<span class="w"> </span>Your<span class="w"> </span>ONNX<span class="w"> </span>model<span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>generated<span class="w"> </span>with<span class="w"> </span>INT64<span class="w"> </span>weights,<span class="w"> </span><span class="k">while</span><span class="w"> </span>TensorRT<span class="w"> </span>does<span class="w"> </span>not<span class="w"> </span>natively<span class="w"> </span>support<span class="w"> </span>INT64.<span class="w"> </span>Attempting<span class="w"> </span>to<span class="w"> </span>cast<span class="w"> </span>down<span class="w"> </span>to<span class="w"> </span>INT32.
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>onnx2trt_utils.cpp:246:<span class="w"> </span>One<span class="w"> </span>or<span class="w"> </span>more<span class="w"> </span>weights<span class="w"> </span>outside<span class="w"> </span>the<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>INT32<span class="w"> </span>was<span class="w"> </span>clamped
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>onnx2trt_utils.cpp:246:<span class="w"> </span>One<span class="w"> </span>or<span class="w"> </span>more<span class="w"> </span>weights<span class="w"> </span>outside<span class="w"> </span>the<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>INT32<span class="w"> </span>was<span class="w"> </span>clamped
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>onnx2trt_utils.cpp:246:<span class="w"> </span>One<span class="w"> </span>or<span class="w"> </span>more<span class="w"> </span>weights<span class="w"> </span>outside<span class="w"> </span>the<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>INT32<span class="w"> </span>was<span class="w"> </span>clamped
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>onnx2trt_utils.cpp:246:<span class="w"> </span>One<span class="w"> </span>or<span class="w"> </span>more<span class="w"> </span>weights<span class="w"> </span>outside<span class="w"> </span>the<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>INT32<span class="w"> </span>was<span class="w"> </span>clamped
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:47<span class="o">]</span><span class="w"> </span><span class="o">[</span>W<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>Output<span class="w"> </span><span class="nb">type</span><span class="w"> </span>must<span class="w"> </span>be<span class="w"> </span>INT32<span class="w"> </span><span class="k">for</span><span class="w"> </span>shape<span class="w"> </span>outputs
<span class="o">[</span><span class="m">09</span>/23/2021-10:20:56<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>Some<span class="w"> </span>tactics<span class="w"> </span><span class="k">do</span><span class="w"> </span>not<span class="w"> </span>have<span class="w"> </span>sufficient<span class="w"> </span>workspace<span class="w"> </span>memory<span class="w"> </span>to<span class="w"> </span>run.<span class="w"> </span>Increasing<span class="w"> </span>workspace<span class="w"> </span>size<span class="w"> </span>may<span class="w"> </span>increase<span class="w"> </span>performance,<span class="w"> </span>please<span class="w"> </span>check<span class="w"> </span>verbose<span class="w"> </span>output.
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:32<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span><span class="o">[</span>TRT<span class="o">]</span><span class="w"> </span>Detected<span class="w"> </span><span class="m">1</span><span class="w"> </span>inputs<span class="w"> </span>and<span class="w"> </span><span class="m">6</span><span class="w"> </span>output<span class="w"> </span>network<span class="w"> </span>tensors.
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:33<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Starting<span class="w"> </span>inference<span class="w"> </span>threads
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Warmup<span class="w"> </span>completed<span class="w"> </span><span class="m">0</span><span class="w"> </span>queries<span class="w"> </span>over<span class="w"> </span><span class="m">200</span><span class="w"> </span>ms
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Timing<span class="w"> </span>trace<span class="w"> </span>has<span class="w"> </span><span class="m">0</span><span class="w"> </span>queries<span class="w"> </span>over<span class="w"> </span><span class="m">3</span>.01861<span class="w"> </span>s
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Trace<span class="w"> </span>averages<span class="w"> </span>of<span class="w"> </span><span class="m">10</span><span class="w"> </span>runs:
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Average<span class="w"> </span>on<span class="w"> </span><span class="m">10</span><span class="w"> </span>runs<span class="w"> </span>-<span class="w"> </span>GPU<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.6003<span class="w"> </span>ms<span class="w"> </span>-<span class="w"> </span>Host<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.7851<span class="w"> </span>ms<span class="w"> </span><span class="o">(</span>end<span class="w"> </span>to<span class="w"> </span>end<span class="w"> </span><span class="m">11</span>.8375<span class="w"> </span>ms,<span class="w"> </span>enqueue<span class="w"> </span><span class="m">6</span>.83557<span class="w"> </span>ms<span class="o">)</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Average<span class="w"> </span>on<span class="w"> </span><span class="m">10</span><span class="w"> </span>runs<span class="w"> </span>-<span class="w"> </span>GPU<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.0905<span class="w"> </span>ms<span class="w"> </span>-<span class="w"> </span>Host<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.2746<span class="w"> </span>ms<span class="w"> </span><span class="o">(</span>end<span class="w"> </span>to<span class="w"> </span>end<span class="w"> </span><span class="m">11</span>.2852<span class="w"> </span>ms,<span class="w"> </span>enqueue<span class="w"> </span><span class="m">6</span>.02471<span class="w"> </span>ms<span class="o">)</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Average<span class="w"> </span>on<span class="w"> </span><span class="m">10</span><span class="w"> </span>runs<span class="w"> </span>-<span class="w"> </span>GPU<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.0689<span class="w"> </span>ms<span class="w"> </span>-<span class="w"> </span>Host<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.2532<span class="w"> </span>ms<span class="w"> </span><span class="o">(</span>end<span class="w"> </span>to<span class="w"> </span>end<span class="w"> </span><span class="m">11</span>.2637<span class="w"> </span>ms,<span class="w"> </span>enqueue<span class="w"> </span><span class="m">5</span>.55458<span class="w"> </span>ms<span class="o">)</span>
<span class="o">[</span><span class="m">09</span>/23/2021-10:24:36<span class="o">]</span><span class="w"> </span><span class="o">[</span>I<span class="o">]</span><span class="w"> </span>Average<span class="w"> </span>on<span class="w"> </span><span class="m">10</span><span class="w"> </span>runs<span class="w"> </span>-<span class="w"> </span>GPU<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.1319<span class="w"> </span>ms<span class="w"> </span>-<span class="w"> </span>Host<span class="w"> </span>latency:<span class="w"> </span><span class="m">11</span>.3166<span class="w"> </span>ms<span class="w"> </span><span class="o">(</span>end<span class="w"> </span>to<span class="w"> </span>end<span class="w"> </span><span class="m">11</span>.3275<span class="w"> </span>ms,<span class="w"> </span>enqueue<span class="w"> </span><span class="m">6</span>.30752<span class="w"> </span>ms<span class="o">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          
<html>
<head>
 
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="keywords" content="footer, address, phone, icons" />
 
	<title>Footer With Address And Phones</title>
	
	
 
</head>
 
	<body>
		
		<footer class="footer-distributed">
 
		<div class="footer-left">
 
		<p class="footer-links">
		<a href="index.html">Home</a>
	·
		<a href="http://www.o3r.ifm" target="_blank">Learn more</a>
	·
		<a href="/latest/_static/privacy-policy.html" target="_blank">Privacy Policy</a>

		</p>
 
		<p class="footer-company-name">ifm robotics &copy; 2023</p>
		</div>
		
		<div class="footer-center">
 
		<div>
		<i class="fa fa-envelope"></i>
		<p><a href="mailto:support.efector.object-ident@ifm.com">support.efector.object-ident@ifm.com</a></p>
		</div>
 
		</div>
 
		<div class="footer-right">
 
		<div class="footer-icons">
 
		<a href="https://www.facebook.com/ifmefector" target="_blank"><i class="fa fa-facebook"></i></a>
		<a href="https://twitter.com/ifm_USA" target="_blank"><i class="fa fa-twitter"></i></a>
		<a href="https://www.linkedin.com/showcase/ifm-efector" target="_blank"><i class="fa fa-linkedin"></i></a>
		<a href="https://github.com/ifm" target="_blank"><i class="fa fa-github"></i></a>
 
		</div>
 
		</div>
 
		</footer>
 
	</body>
 
</html>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>